{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"covid_data/\"\n",
    "normal_path= data_path + '0/'\n",
    "covid_path = data_path + '1/'\n",
    "\n",
    "train_filenames = [] # all image name\n",
    "train_labels = [] # all image labels\n",
    "\n",
    "# label o for normal\n",
    "for filename in os.listdir(normal_path):\n",
    "    train_filenames.append(normal_path + filename)\n",
    "    train_labels.append(0)\n",
    "\n",
    "# label 1 for covid\n",
    "for filename in os.listdir(covid_path):\n",
    "    train_filenames.append(covid_path + filename)\n",
    "    train_labels.append(1)\n",
    "\n",
    "\n",
    "# Random data splitting\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_filenames, train_labels, test_size=0.2)\n",
    "\n",
    "# covert to tensorflow constant\n",
    "train_filenames = tf.constant(X_train)\n",
    "valid_filenames = tf.constant(X_valid)\n",
    "train_labels = tf.constant(y_train)\n",
    "valid_labels = tf.constant(y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function help to decode and resize the image to resized img and its label\n",
    "def decode_and_resize(filename, label):\n",
    "    image_string = tf.io.read_file(filename)            \n",
    "    image_decoded = tf.image.decode_jpeg(image_string) \n",
    "    image_resized = tf.image.resize(image_decoded, [256, 256]) / 255.0 # reset size，and normalized\n",
    "    return image_resized, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_filenames, train_labels))\n",
    "train_dataset = train_dataset.map(map_func=decode_and_resize, # call decode_and_resize method and covert file to features and labels\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(32) # amount of data per batch\n",
    "\n",
    "# valid_dataset\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_filenames, valid_labels))\n",
    "valid_dataset = valid_dataset.map(\n",
    "    map_func=decode_and_resize,\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Convolutional Layer，32 filter，3*3，strides is 1\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(256, 256, 3)),\n",
    "    # Pooling Layer，2*2，trides are 2\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 254, 254, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 123, 123, 32)      25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 119072)            0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                7620672   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 7,647,330\n",
      "Trainable params: 7,647,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 338ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.4510\n",
      "[0.6959580183029175, 0.4248788356781006]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((valid_filenames, valid_labels))\n",
    "test_dataset = test_dataset.map(_decode_and_resize)\n",
    "test_dataset = test_dataset.batch(32)\n",
    "\n",
    "print(model.evaluate(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'sparse_categorical_accuracy']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.metrics_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
